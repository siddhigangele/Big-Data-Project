{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-2.4.7-bin-hadoop2.7'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('C:\\spark\\spark-2.4.7-bin-hadoop2.7')\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Classification with Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"data_clean.csv\",header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+------------------------+--------+-----------------+----------------------+------------------+--------------------+\n",
      "|      Summary|  Temperature (C)|Apparent Temperature (C)|Humidity|Wind Speed (km/h)|Wind Bearing (degrees)|   Visibility (km)|Pressure (millibars)|\n",
      "+-------------+-----------------+------------------------+--------+-----------------+----------------------+------------------+--------------------+\n",
      "|Partly Cloudy|9.472222222222221|       7.388888888888887|    0.89|          14.1197|                 251.0|15.826300000000002|             1015.13|\n",
      "|Partly Cloudy|9.355555555555558|       7.227777777777777|    0.86|          14.2646|                 259.0|15.826300000000002|             1015.63|\n",
      "|Mostly Cloudy|9.377777777777778|       9.377777777777778|    0.89|           3.9284|                 204.0|           14.9569|             1015.94|\n",
      "|Partly Cloudy| 8.28888888888889|       5.944444444444446|    0.83|          14.1036|                 269.0|15.826300000000002|             1016.41|\n",
      "|Mostly Cloudy|8.755555555555553|       6.977777777777778|    0.83|          11.0446|                 259.0|15.826300000000002|             1016.51|\n",
      "+-------------+-----------------+------------------------+--------+-----------------+----------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summary: string (nullable = true)\n",
      " |-- Temperature (C): double (nullable = true)\n",
      " |-- Apparent Temperature (C): double (nullable = true)\n",
      " |-- Humidity: double (nullable = true)\n",
      " |-- Wind Speed (km/h): double (nullable = true)\n",
      " |-- Wind Bearing (degrees): double (nullable = true)\n",
      " |-- Visibility (km): double (nullable = true)\n",
      " |-- Pressure (millibars): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+------------------------+--------+-----------------+----------------------+---------------+--------------------+\n",
      "|Summary|Temperature (C)|Apparent Temperature (C)|Humidity|Wind Speed (km/h)|Wind Bearing (degrees)|Visibility (km)|Pressure (millibars)|\n",
      "+-------+---------------+------------------------+--------+-----------------+----------------------+---------------+--------------------+\n",
      "|      0|              0|                       0|       0|                0|                     0|              0|                   0|\n",
      "+-------+---------------+------------------------+--------+-----------------+----------------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "#checking for null ir nan type values in our columns\n",
    "dataset.select([count(when(col(c).isNull(), c)).alias(c) for c in dataset.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol):\n",
    "\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "                 for c in categoricalCols ]\n",
    "\n",
    "    # default setting: dropLast=True\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "                 outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n",
    "                 for indexer in indexers ]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\n",
    "                                + continuousCols, outputCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "\n",
    "    data = data.withColumn('label',col(labelCol))\n",
    "\n",
    "    return data.select(indexCol,'features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[1:]),r[0]]).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|            features|        label|\n",
      "+--------------------+-------------+\n",
      "|[9.47222222222222...|Partly Cloudy|\n",
      "|[9.35555555555555...|Partly Cloudy|\n",
      "|[9.37777777777777...|Mostly Cloudy|\n",
      "|[8.28888888888889...|Partly Cloudy|\n",
      "|[8.75555555555555...|Mostly Cloudy|\n",
      "+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = transData(dataset)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------------+\n",
      "|            features|        label|indexedLabel|\n",
      "+--------------------+-------------+------------+\n",
      "|[9.47222222222222...|Partly Cloudy|         0.0|\n",
      "|[9.35555555555555...|Partly Cloudy|         0.0|\n",
      "|[9.37777777777777...|Mostly Cloudy|         1.0|\n",
      "|[8.28888888888889...|Partly Cloudy|         0.0|\n",
      "|[8.75555555555555...|Mostly Cloudy|         1.0|\n",
      "+--------------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index labels, adding metadata to the label column\n",
    "labelIndexer = StringIndexer(inputCol='label',\n",
    "                             outputCol='indexedLabel').fit(transformed)\n",
    "labelIndexer.transform(transformed).show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+\n",
      "|            features|        label|     indexedFeatures|\n",
      "+--------------------+-------------+--------------------+\n",
      "|[9.47222222222222...|Partly Cloudy|[9.47222222222222...|\n",
      "|[9.35555555555555...|Partly Cloudy|[9.35555555555555...|\n",
      "|[9.37777777777777...|Mostly Cloudy|[9.37777777777777...|\n",
      "|[8.28888888888889...|Partly Cloudy|[8.28888888888889...|\n",
      "|[8.75555555555555...|Mostly Cloudy|[8.75555555555555...|\n",
      "+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =VectorIndexer(inputCol=\"features\", \\\n",
    "                                outputCol=\"indexedFeatures\", \\\n",
    "                                maxCategories=4).fit(transformed)\n",
    "featureIndexer.transform(transformed).show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|            features|        label|\n",
      "+--------------------+-------------+\n",
      "|[-16.666666666666...|        Clear|\n",
      "|[-15.555555555555...|        Clear|\n",
      "|[-15.483333333333...|Partly Cloudy|\n",
      "|[-15.0,-22.738888...|        Clear|\n",
      "|[-14.977777777777...|        Foggy|\n",
      "+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[-16.666666666666...|Clear|\n",
      "|[-15.733333333333...|Clear|\n",
      "|[-15.466666666666...|Clear|\n",
      "|[-15.0,-15.0,0.0,...|Clear|\n",
      "|[-15.0,-15.0,0.83...|Foggy|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (40% held out for testing)\n",
    "(trainingData, testData) = transformed.randomSplit([0.6, 0.4])\n",
    "\n",
    "trainingData.show(5)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf,labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------+\n",
      "|            features|label|predictedLabel|\n",
      "+--------------------+-----+--------------+\n",
      "|[-16.666666666666...|Clear| Mostly Cloudy|\n",
      "|[-15.733333333333...|Clear| Mostly Cloudy|\n",
      "|[-15.466666666666...|Clear| Mostly Cloudy|\n",
      "|[-15.0,-15.0,0.0,...|Clear| Partly Cloudy|\n",
      "|[-15.0,-15.0,0.83...|Foggy|         Foggy|\n",
      "+--------------------+-----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "# Select example rows to display.\n",
    "predictions.select(\"features\",\"label\",\"predictedLabel\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.508185\n",
      "RandomForestClassificationModel (uid=RandomForestClassifier_73100c9abb9c) with 100 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[-2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
